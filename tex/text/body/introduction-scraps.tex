Digital evolution is a field of research that sits at the intersection of evolutionary biology and machine learning.
By instantiating evolution in computers, researchers are able to advance our knowledge of both subjects.
Increasing computational power is essential to many advances in digital evolution and, more broadly, artificial life \citep{ackley2014indefinitely}.
In fact, a central motif in the field --- the problem of open-ended evolution, which asks how and if closed systems can indefinitely yield new complexity, novelty, and adaptation --- is inexorably intertwined with computational scale, and has been implicated as meaningfully compute-constrained in at least some systems \citep{taylor2016open,channon2019maximum}.
It is not an unreasonable possibility that orders-of-magnitude changes in computational power could induce qualitative changes in digital evolution and artificial life systems \citep{moreno2022engineering}, analogously to the renaissance of deep learning with the advent of GPGPU computing \citep{krizhevsky2012imagenet}.

However, contemporary advances in computer hardware have been a double-edged sword.
Fundamental physical limitations have essentially curtailed transformative progress in individual processing elements \citep{sutter2005free}.
Instead, the frontier of raw compute power in high-performance computing (HPC) has largely been advanced by increasing processing core counts \citep{morgenstern2021unparalleled}.
Leveraging parallel and distributed computing power for digital evolution will require continuing work adapting existing methodologies oriented towards serial processing to suit these hardware trends.

% On the flip side, economization and miniaturization has driven broad dissemination of computing capability through the Internet of Things (IoT) revolution \citep{rfc7228,ojo2018review}.
% A notable pattern cuts across some facets of both fringes: absolute reduction or tightening or relative tightening in resources and capabilities per processing element, particularly with respect to the capacity, latency, or bandwidth of memory and storage.
% Along the march to exascale HPC, processing capacity growth (e.g., FLOPS) has outpaced memory performance and capacity --- particularly with respect to hardware accelerators \citep{kogge2013exascale,khan2021analysis}.
% This ``memory wall'' bottlenecks throughput unless memory access volume is kept sufficiently low in proportion to processing workload (i.e., high ``arithmetic'' or ``operational'' intensity is achieved) \citep{obviousmemorywall,mckee2004reflections,ROOFLINEMODEL2008Berkley}.
% Analogous performance trajectories have played out with respect to data storage \citep{heldens2020landscape,kunkel2014exascale}, imposing an analogous ``storage wall'' bottleneck \citep{hu2016storage}.
% Recent introduction of the Cerebras Wafer-Scale Engine epitomizes the double-edged nature of recent HPC trends: this accelerator packages an astounding 850,000 computing elements onto a single piece of silicon, but restricts endows each with a modest 48kb of memory and provisions an exclusively-local mesh communication model \citep{cerebras2021wafer,lauterbach2021path}.

% The intersection of these observations is that despite relentless technological advancement, severely resource-limited computation will persist.
% In fact, in some domains technological progress has actually elevated the relevance of resource-limited computation.
% These trends, and the wider context of surging generation of digital data \citep{bhat2018data}, demand attention to algorithmic strategies to triage memory/storage-capacity gaps.

% ELD: Moving this to beginning
% Burgeoning computational power has been broadly envisaged as driving force in advancing the field of digital evolution and, more broadly, artificial life \citep{ackley2014indefinitely}.
% In fact, a central motif in the field --- the problem of open-ended evolution, how and if closed systems can yield indefinitely yield new complexity, novelty, and adaptation --- is inexorably intertwined with computational scale, and has been implicated as meaningfully compute-constrained in at least some systems \citep{taylor2016open,channon2019maximum}.
% It is not an unreasonable possibility that orders-of-magnitude changes in computational power could induce qualitative changes in digital evolution and artificial life systems \citep{moreno2022engineering}, analogously to the renaissance of deep learning with the advent of GPGPU computing \citep{krizhevsky2012imagenet}.
% To accomplish such, however, will require contending with scaling challenges induced by aforementioned hardware trends.

% ELD: TODO: consider cutting this paragraph
Parallel and distributed computing is, indeed, already widely leveraged in digital evolution.
Applications range from processing entirely independent evolutionary replicates across compute jobs \citep{dolson2017spatial, hornby2006automated}, to data-parallel fitness evaluation of single individuals over independent test cases using hardware accelerators \citep{harding2007fast_springer, langdon2019continuous}, to application of primary-subordinate/controller-responder parallelism to delegate costly fitness evaluations of single individuals \citep{cantu2001master,miikkulainen2019evolving}.
In recent years, Sentient Technologies spearheaded evolutionary computation projects on an unprecedented computational scale, comprising over a million heterogeneous CPUs from time-available providers capable of a peak performance of 9 petaflops \citep{miikkulainen2019evolving,gilbert2015artificial,blondeau2009distributed}.

Although highlighted as key to the future of the field, fully decentralized, highly-distributed approaches such as island models \citep{bennett1999building,schulte2010genetic} and mesh-computing approaches prioritizing dynamic interactions \citep{ray1995proposal,ackley2018digital,moreno2021conduit} have been less thoroughly developed.
A major barrier to effective applications of the fully-distributed paradigm is the difficulty of observing the state of the system. 
In the absence of a global population-level view, the course of evolution becomes much more challenging to study --- undercutting the objective of these systems as an experimental tool.
A prime example of this problem is the difficulty of recording phylogenies (i.e. ancestry trees) in distributed systems.

\subsection{Phylogeny recording}

Although phylogenetic analysis is a powerful tool for understanding digital evolution \citep{dolson2020interpreting}, typical approaches are particularly difficult to scale.
Most existing analyses use direct lineage tracking, where a centralized data structure collates individual reproduction events to produce a complete, perfect phylogenetic record \citep{dolson2023phylotrackpy}.
However, this approach requires centralized data storage, which performs poorly in distributed computing environments (see Sections \ref{sec:perfect-tracking-distrbuted} and \ref{sec:perfect-tracking-pruning-distrbuted} for more information).

The problem of recording phylogenies in a distributed context generalizes beyond digital evolution.
Phylogenies can be used to understand any process in which a population of objects exist and those objects are sometimes replicated (with or without modification).
Indeed, phylogenetic techniques have been used to study the routes through which digital image misinformation and computer viruses spread \citep{friggeri2014rumor,cohen1987computer}.
As in digital evolution, such studies generally rely on centralized tracking and are thus inhibited when it is not possible.

% As such, hereditary stratigraphic techniques certainly possess some potential afford experimenters visibility into the otherwise cloistered processes through which such digital artifacts spread.
% Notably, though, such experiments would predicate on some level of influence over the artifact copy process in to ensure dispatch of the hereditary stratigraphic update process and the absence of actors with motivation and capability to antagonistically deface annotation data.


%TODO we could probably add some citations/examples about why phylogenetic tracking is important
% AND how perfect phylogenetic tracking is super powerful at doing otherwise unimaginable analyses
% probably pilfer phylotrackpy JOSS paper for ideas
% TODO also tie in the algorithmic analysis of phylogenies here
% ELD: TODO: See if any of the points below aren't made in the perfect tracking section, and if not move them
% However, long distance, potentially many-hop, transmission of reproduction event records to a centralized data store introduces potentially significant amounts of communication overhead and bottlenecking.
% The necessity to propagate extinction notifications at runtime so that records of lineages without extant descendants can be pruned away to prevent runaway memory use imposes additional communication overhead.
% Further, at large enough scales, node failures become inevitablies rather than inconveniences and, at even larger scales, the cost of attempting full recovery from node dropout to prevent data loss could become high enough to conceivably preclude forward simulation progress \citep{cappello2014toward}.
% Within the perfect-tracking paradigm, individual missing relationships could potentially entirely disjoin knowledge of how large portions of phylogenetic history relate.
% This sensitivity to data loss poses a second major challenge in scaling perfect tracking.

% Looking to natural systems, however, fruitful efforts across generations of systematic biologists evidence the viability of robust phylogenetic analyses from data generated through fully-distributed processes.
Looking to natural systems, it is clear that robust phylogenetic analyses can be conducted on data generated through fully-distributed processes.
% An alternate approach to studying phylogenetic dynamics in distributed systems, therefore, can be entertained: post-hoc inference from heritable data rather than centralized tracking.
Inspired by systematic biology, we can consider post-hoc inference from heritable data as an alternative tocentralized tracking.
% Although such analyses can passably be accomplished solely from direct analysis of pre-existing, functional digital genome contents \citep{moreno2021case}, such algorithms do not generalize across diverse digital evolution systems and suffer from many of the complications that plague phylogenetic analyses of biological systems.
Although such analyses can be accomplished solely from pre-existing, functional digital genome contents \citep{moreno2021case}, such algorithms do not generalize across diverse digital evolution systems and suffer from many of the complications that plague phylogenetic analyses of biological systems.
An intriguing proposition arises: how might a genetic representation be designed to maximize ease and efficacy of phylogenetic reconstruction?

If sufficiently compact, such a representation could be attached as a non-functional annotation to facilitate phylogenetic inference over genomes of interest.
This line of reasoning led to the recent development of ``hereditary stratigraphy'' methodology, which associates digital genomes with heritable annotations specially designed to provide universal system-invariant phylogenetic reconstruction with well-defined, tunable expectations for inference accuracy
\citep{moreno2022hereditary}.

%TODO should this whole bit move to methods?
% ELD: Yeah I think so
% The principal aspects of hereditary stratigraphy can be sketched briefly.
% However, briefly sketching a more obvious alternate approach provides a point of reference more familiar to most readers' intuiition.
% A simple heritable annotation to enable relatedness estimation would be a fixed--length bitstring under neutral drift by bit flip mutation.
% As lineages diverge from a common ancestor, gradual mutational accumulation probabilistically causes the hamming distance between annotations to diverge.
% Therefore, hamming distance between these annotations can be used as inferential proxy: more mismatching sites imply less relatedness between two annotated digital organisms.
% Unfortunately, this model suffers from several serious drawbacks in application as a tool for relatedness estimation, including difficulty resolving uneven lineage branch lengths and difficulty parameterizing the model for effective inference over greatly varying generational scales.

% Hereditary stratigraphy annotations employ a systematic update process at generational turnovers instead of a stochastic mutational process as described above.
% Each generation, a new fixed-length packet of randomly-generated data is appended to inherited annotations.
% These stochastic ``fingerprints,'' typically sized on the order of a few bits or bytes, serve as a sort of checkpoint for lineage identity at a particular generation.
% At future time points, extant annotations will share identical fingerprints for the generations they experienced shared ancestry.
% Estimation of the last common ancestor generation is straightforward: the first mismatching fingerprints denote the end of common ancestry.
% However, note that overestimation of relatedness can occur due to spurious fingerprint collision events (i.e., where strata happen to share the same randomly-generated value by chance); the probability of such events depends on ``fingerprint'' width.
% The nomenclature of ``hereditary stratigraphy'' was chosen through analogy to geological layering processes.

\subsection{Hereditary stratigraphy}

The crux of hereditary stratigraphy is that each at generation, a new fixed-length packet of randomly-generated data is appended to inherited annotations.
These stochastic ``fingerprints,'' typically sized on the order of a few bits or bytes, serve as a sort of checkpoint for lineage identity at a particular generation.
At future time points, extant annotations will share identical fingerprints for the generations they experienced shared ancestry.

Interestingly, a similar technique was used by \cite{libennowell2008tracing} to trace global dissemination of chain emails.
These researchers applied \textit{post-hoc} methods to reconstruct estimated phylogenies of the propagation of two chain mail messages.
These phylogenies then served as a reference to tune agent-based models, ultimately yielding a remarkable elucidation of email user behavior and underlying social dynamics.
Interestingly, this study's reconstructions were solely enabled by a special peculiarity of the two sampled messages: they were email petitions.
Thus, users would append their name to the list before forwarding it on --- a mechanism strikingly similar to the broad strokes of hereditary stratigraphy.

Naive application of hereditary stratigraphy would produce indefinitely lengthening genome annotation size in direct proportion to generations elapsed.
To overcome this obstacle, hereditary stratigraphy prescribes a ``pruning'' process to discard strata on the fly as new strata are deposited.
Pruning reduces annotation size, but at the expense of introducing inferential uncertainty: the last generation of common ancestry between two lineages can be resolved no finer than the time points associated with retained strata.
Deciding which strata to discard is the main point of algorithmic interest associated with the technique.
In fact, this scenario turns out to be an instance of a more general class of online algorithmic problems we hereby term as ``streaming curation.''
In the context of hereditary stratigraphy, we refer to the decision-making rules that decide which retained strata to discard per generation as a ``stratum retention'' algorithm.
% ELD: TODO: is this correct or do we call them curation policy algorithms now?

% Putting ``hereditary stratigraphy'' techniques into practice requires a point of further consideration.
% Naive application of the approach described above would entail indefinitely lengthening genome annotation size in direct proportion to generations elapsed due to the creation of new ``fingerprint'' data each generation.
% To overcome this obstacle, hereditary stratigraphy prescribes a ``pruning'' process to discard strata on the fly as new strata are deposited.
% Note that pruning reduces annotation size, but at the expense of introducing inferential uncertainty: the last generation of common ancestry between two lineages can be resolved no finer than the time points associated with retained strata.
% Deciding which strata to discard when proves to be the main point of algorithmic interest associated with the technique.
% In fact, as will be delved into a little further on, we find this scenario to be an instance of a more general class of online algorithmic problems we term as ``streaming curation.''
% In the context of hereditary stratigraphy, we refer to the decision-making rules that decide which retained strata to discard per generation as a ``stratum retention'' algorithm.


\subsection{Stratum retention}

The obvious core issue of stratum retention is how many strata to discard.
In many applied cases, it is desirable to keep the count of retained strata at or below a size cap indefinitely as generations elapse.
However, in some cases discussed later on, it may be desirable to allow a logarithmic growth rate of annotation size to guarantee upper bounds on inferential uncertainty.

Determining stratum retention strategy also raises a more subtle second consideration: what skew, if any, to induce on the composition of retained strata.
Strata from evenly-spaced time points may be retained in order to provide uniform inferential detail over the entire range of elapsed time points.
However, coalescent theory predicts that evolution-like processes will tend to produce phylogenies with many recent branches and progressively fewer more ancient branches \citep{nordborgCoalescentTheory2019, berestyckiRecentProgressCoalescent2009}.
Thus, fine inferential detail over more recent time points is usually more informative to phylogenetic reconstruction than fine detail over more ancient time points.
Thus, among a fixed-size retained sampling of time points, skewing the composition of retained strata to over-represent more recent time points would likely provide better bang for the buck with respect to reconstructive power.
Indeed, experiments reconstructing known lineages have shown that recency-skewing retention provides better quality reconstructions \citep{moreno2022hereditary}.
So, in addition to evenly-spaced retention, we consider retention allocations that yield gap widths between successive retained strata (and corresponding estimation uncertainty) scaled proportionately to those strata's depth back in history.

Because no single retention policy can meet requirements of all use cases, we present in Section \ref{sec:annotation-algorithms} a suite of complementary stratum retention algorithms covering possible combinations of retained stratum count order of growth and chronological skew.
We describe retention algorithms in terms of upper bounds on memory usage and inference uncertainty.

With respect to memory usage, we refer to guaranteed upper bounds as ``size order of growth'' in the asymptotic case with respect to elapsed generations or ``size cap'' in the absolute case.
We refer to bounds on spacing between retained strata a ``resolution guarantee.''
Resolution guarantee specification incorporates both the total number of generations elapsed and the historical depth of a particular time point in the stratigraphic record --- so, bounding is tailored within these particular circumstances.

%ELD: TODO: This said "three nutes and bolts consdierations" but there were only two. I changed it to say two, but is there one missing?
In addition to their particular size bounds and resolution guarantees, we demonstrate all proposed stratum retention algorithms satisfy two nuts and bolts algorithmic considerations:
\begin{enumerate}
\item \textbf{Tractability of directly enumerating deposition time of retained strata at any arbitrary generation.} Efficient computation of the deposition times retained at each time point provides a tractable reverse mapping from column array index to deposition generation.
  Such a mapping enables deposition generation to be omitted from stored strata, potentially yielding several-fold space savings (depending on the differentia bit width used).

\item \textbf{Stratum discard sequencing for ``self-consistency.''} 
  When you discard a stratum now, it won't be available later.
  If you need a stratum at a particular time point, you must be able to guarantee it hasn't already been discarded at any prior time point.

\end{enumerate}

% As a final spelling out of a critical conceptual point in discussion of stratum retention policies, note that size bounds and resolution guarantees are enforced across across all generations.
% This means stratum retention policies must manage ``online'' column composition across rolling generations.
% Indeed, for many use cases, resolution and column size guarantees will need to hold at all generations because the number of generations elapsed at the end of an experiment is often not known \textit{a priori} and the option of continuing a fixed-length experiment with evolved genomes is desired.
% This factor introduces a design subtlety: as generations elapse, deposited strata recede to increasingly ancient historical depth with respect to the current generation.
% Resolution guarantees may change along the way back.
% In those cases, cohorts of retained strata must, in dwindling, gracefully morph through a constrained series of retention patterns.

Note that size bounds and resolution guarantees are enforced across across all generations.
Thus, stratum retention policies must manage ``online'' column composition across rolling generations.
Indeed, for many use cases, resolution and column size guarantees will need to hold at all generations because the number of generations elapsed at the end of an experiment is often not known \textit{a priori} and the option of continuing a fixed-length experiment with evolved genomes is desired.
This factor introduces a design subtlety: as generations elapse, deposited strata recede to increasingly ancient historical depth with respect to the current generation.
Resolution guarantees may change along the way back.
In those cases, cohorts of retained strata must, in dwindling, gracefully morph through a constrained series of retention patterns.

\subsection{The streaming curation problem}

Stepping back, the online filtering obligations faced by hereditary stratigraph annotations are not unlike those faced by unattended data logger devices.
Both manage incoming observation streams on a potentially indefinite or indeterminate basis and both operate under storage space limitations.
Further, both are presumedly tasked to operate under some stipulation for time coverage, whether simply rolling full retention of most recent data within available buffer space \citep{fincham1995use}, dismissal of incoming data after storage reaches capacity \citep{saunders1989portable,mahzan2017design}, best-effort even coverage of the elapsed period, or otherwise.
Even high-capacity devices may experience overflow conditions when confronted with high-frequency data streams \citep{luharuka2003design}.

Restated explicitly, these scenarios confront a question of how to satisfactorily maintain a temporally representative cross section of observations that stream in on a rolling basis.
We propose this problem as the ``stream curation problem.''
For the sake of generalizability, Section \ref{sec:annotation-algorithms} organizes presentation of stratum retention algorithms through the lens of the streaming curation problem.
There has been some work to extend the record capacity of data loggers through application-specific online compression algorithms \citep{hadiatna2016design}, but to our knowledge the streaming curation problem has not been treated directly from either a theoretical or empirical vantage.
As low-end hardware continues to march toward the downscale and ubiquity of ``smart dust'' \citep{warneke2001smart}, we expect this problem will become increasingly relevant.

% Indeed, low-end hardware continues to march toward the downscale and ubiquity of ``smart dust'' \citep{warneke2001smart}.
% The ``Michigan Micro Mote'' constitutes a notable waypoint \citep{lee2012modular}.
% These devices occupy only a cubic millimeter of volume, but provision a mere 3 kb of retentive memory.
% Introduced in 2012, applications have since been explored across imaging, motion detection, pressure measurement, and electrocardiography monitoring \citep{lee2015review}.
% Taking for instance another more recent high-profile project, computational capacity was recently tucked into wireless sensor network devices adopting the form factor of dandelion parachutes \citep{iyer2022wind}.
% The chipset for these devices provisions 2 kilobytes of volatile flash memory --- and a mere 128 bytes of retentive memory \citep{microchip2014atiny20}.
% Even more striking is the advent of an epidermal physiological sensor equipped with carbon nanotube-based flash memory of a scant 48 bytes \citep{xiang2022epidermal}.
% The ever abiding race to plumb the very lower limits of technical feasibility seems likely to keep bare-bones computing modalities front and center within this engineering domain far into the future.

% Recalling earlier discussion of hardware trends, the accelerating ubiquitization of IoT hardware and, in particular, continuing downscaling toward smart dust call stream curation scenarios directly into play.

Some of the same curation policy algorithms we propose for stratum retention could also be useful in these cases.
For example, organization of IoT devices into wireless sensor networks is expected, in a considerable fraction of cases, to structure irregular device uplink schedules, such as the ``mobile sink'' paradigm \citep{jain2022survey}.
% The so-called ``mobile sink'' paradigm has risen to some prominence due to advantages with respect to power management, security, and network load management \citep{jain2022survey}.
Under this model, network base station(s) physically traverse the coverage area and transact with nearby sensor nodes.
Reliance on the mobile sink's patrol schedule potentially introduces uncertainty in data transfer schedules.
% TODO: One sentence on why our curation policies would help with this
Streaming curation may be particularly relevant to data management in surveillance devices such as doorbell or body-worn cameras, which have experienced notable market growth \citep{calacci2022cop,lum2019research}.
Recency-proportional retention may suit some applications, where time intervals of interest may be flagged well after the fact, but tend to bias toward the recent past.
% This approach could also align with the objectives of some accident recording devices.
% TODO this sentence sucks: rephrase
Finally, streaming curation may even pertain to record management in large capacity centralized storage systems in some scenarios \citep{bhat2018data}.

% It should also be noted that beyond connections to streaming curation, hereditary stratigraphy itself bears upon applications outside digital evolution modeling.
% Computer viruses and digital media also proliferate through replication, often outside the scope of direct centralized observability.
% Furthermore, understanding the dynamics of these phenomena is of significant scientific and societal interest \citep{aslan2020comprehensive,dupuis2019spread,ling2021dissecting}.
% Indeed, some research has reported on the routes through which digital image misinformation and computer viruses spread \citep{friggeri2014rumor,cohen1987computer}.
% Such studies generally rely on centralized tracking, which is not possible in many circumstances.
% A notable exception is work by \cite{libennowell2008tracing} to trace global dissemination of chain emails.
% These researchers applied \textit{post-hoc} methods to reconstruct estimated phylogenies of the propagation of two chain mail messages.
% These phylogenies then served as a reference to tune agent-based models, ultimately yielding a remarkable means elucidation of email user behavior and underlying social dynamics.
% Interestingly, this study's reconstructions were solely enabled by a special peculiarity of the two sampled messages: they were email petitions.
% Thus, users would append their name to the list before forwarding it on --- a mechanism strikingly similar to the broad strokes of hereditary stratigraphy.

% As such, hereditary stratigraphic techniques certainly possess some potential afford experimenters visibility into the otherwise cloistered processes through which such digital artifacts spread.
% Notably, though, such experiments would predicate on some level of influence over the artifact copy process in to ensure dispatch of the hereditary stratigraphic update process and the absence of actors with motivation and capability to antagonistically deface annotation data.


\subsection{Hereditary Stratigraphy} \label{sec:hereditary-stratigraphy}

Digital systems excel at copying and distributing information \citep{miller2001taking}.
Provenance among digital artifacts produced through copy operations can be represented as a tree of parent-child relationships (in this case, original-copy relationships).
The structure of digital ancestry trees can answer important questions pertinent to cybersecurity, misinformation, and cultural anthropology \citep{aslan2020comprehensive,dupuis2019spread,ling2021dissecting}.
To explain how and why such ancestry analysis might be accomplished, we next contrast direct ancestry tracking with distributed ancestry tracking, outline hereditary stratigraphy methodology for distributed ancestry tracking, and review applications of distributed ancestry tracking.

\subsubsection{Direct Ancestry Tracking}

Most work on ancestry trees of digital artifacts relies on centralized lineage tracking \citep{friggeri2014rumor,cohen1987computer,dolson2023phylotrackpy}.%
\footnote{A notable exception, \cite{libennowell2008tracing} exploit a serendipitous mechanistic opportunity to reconstruct global dissemination of chain emails --- discussed further below.}
Direct approaches to tracking replicator provenance in digital systems operate on this graph structure directly, distilling it from the full set of parent-child relationships over the history of a population to produce an exact historical account.

Unfortunately, typical lineage tracking is difficult to scale.
At scale, practical limitations preclude comprehensive records of replication events, which accumulate linearly with elapsed generations.
So, extinct lineages must be pruned.
Detecting lineage extinctions requires either (1) collation of all replication and destruction events to a centralized data store or (2) peer-to-peer transmission of extinction notifications that unwind a lineage's (possibly many-hop) trajectory across host nodes.
Both options oblige runtime communication overhead.
To boot, the perfect-tracking paradigm is fragile to single missing relationships --- these can entirely disjoin knowledge of how large portions of phylogenetic history relate.
This fragility makes direct lineage tracking highly sensitive to data loss and dynamic network topology rearrangements, which are ubiquitous at scale \citep{cappello2014toward,ackley2011pursue}.

\subsubsection{Motivation for Decentralized Ancestry Tracking}

Under an alternate framing, biological phylogenetics almost certainly constitutes the single most extensive investigation of artifact provenance ever conducted.
Decades of phylogenetics researchers have waged a vast and sophisticated campaign to stitch together millions of lines of descent via analysis of fossil records, phenotypic traits, and --- especially --- genetic information \citep{hinchliff2015synthesis,lee2015morphological}.
The successes of biological phylogenetics resoundingly demonstrate the viability of robust phylogenetic analyses over large, long-running, and fully-distributed systems.

Inspired by systematic biology, we can consider post hoc inference from heritable data as an alternative to direct tracking.
At core, biological processes of mutation, recombination, conjugation, transformation, transduction, and gene transfer induce patterns of heritable variation that can be mined to reveal historical relationships \citep{davis1992populations}.

Indeed, in digital systems with imperfect copy processes (e.g., digital evolution), phylogenetic reconstructions can be accomplished ad hoc, i.e., directly from copy artifacts themselves \citep{moreno2021case}.
However, this approach has major drawbacks ---
(1) ad hoc reconstruction algorithms are highly dependent on representational specifics so do not readily generalize;
(2) ad hoc approaches suffer from many of the complications that plague phylogenetic analyses of biological systems such as back mutation and mutational saturation;
(3) systems without mutational dynamics (i.e., perfect copies) entirely lack information on heredity.

The challenges associated with ad hoc ancestry estimation begs the question of how to design genetic material to optimize phylogenetic reconstruction.
If sufficiently compact, such a representation could be annotated onto artifacts of interest in order to facilitate phylogenetic inference.
This scheme assigns each artifact an annotation, with parents' annotations passed down to offspring artifact copies.

The recently developed ``hereditary stratigraphy'' methodology implements this approach, providing a design for heritable annotations that facilitate universal system-invariant phylogenetic reconstruction with well-defined, tunable expectations for inference accuracy \citep{moreno2022hereditary}.

\subsubsection{Mechanisms for Decentralized Ancestry Tracking}

The core mechanism of hereditary stratigraphy is accretion, and subsequent inheritance, of a new randomized data packet each copy generation.
These stochastic fingerprints, which we call ``differentia,'' serve as a sort of checkpoint for lineage identity at a particular generation.
At future time points, extant annotations will share identical differentia for the generations they experienced shared ancestry.
So, the first mismatched fingerprint between two annotations bounds their common ancestry.

To circumvent annotation size bloat, hereditary stratigraphy prescribes a ``pruning'' process to delete differentia on the fly as generations elapse.
This pruning, however, comes at a cost to inference power.
The last generation of common ancestry between two lineages can be resolved no finer than retained checkpoint times.
In the context of hereditary stratigraphy, we refer to the procedure for down sampling as a ``stratum retention algorithm'' and the resulting patterns of retained differentia as a ``stratum retention policy.''
Stratum retention algorithms must decide how many records to discard, but also how remaining records distribute over past time.
Discussion considers tuning stratum retention trade-offs elsewhere as an instance of the more general ``stream curation'' problem --- introduced separately below in Section \ref{sec:streaming-curation} and fully presented in Section \ref{sec:annotation-algorithms}:

Procedures for ancestry tree reconstruction from a population of stratigraph annotations follow from the same principle as pairwise annotation comparison, but merit some further elaboration.
Section \ref{sec:reconstruction-algorithm} provides a trie-based algorithm for this task.

\subsubsection{Distributed Ancestry Trees in Digital Evolution}

Digital evolution refers to computational systems that combine replication, mutation, and selection to instantiate an evolutionary process.
The field sits at the intersection of evolutionary biology, viz. simulation experiments, and machine learning, viz. heuristic optimization.
Within digital evolution, complications of distributed tracking have historically restricted most phylogenetic analyses to single-process or centralized leader/follower simulations.
Need for phylogenetic methods compatible with parallel and distributed simulation motivated original development of hereditary stratigraphy \citep{moreno2022hereditary}.

Computational scale matters in digital evolution and, more broadly, artificial life \citep{ackley2014indefinitely} --- particularly with respect to open-ended evolution (OEE), the question of how (and if) closed systems can indefinitely grow in complexity, novelty, and adaptation \citep{taylor2016open}.
OEE is inexorably intertwined with computational scale, and has been implicated as meaningfully compute-constrained in at least some systems \citep{channon2019maximum}.
It is not an unreasonable possibility that orders-of-magnitude changes in computational scale of digital evolution could induce qualitative effects \citep{moreno2022engineering}, analogously to the renaissance of deep learning with the advent of GPGPU computing \citep{krizhevsky2012imagenet}.

% ELD: TODO: consider cutting this paragraph
Indeed, digital evolution work already commonly marshals substantial parallel and distributed computing  --- in some cases reaching petaflop scale \citep{gilbert2015artificial}.
Methods range from entirely independent evolutionary replicates across compute jobs \citep{dolson2017spatial, hornby2006automated}, to data-parallel fitness evaluation of single individuals over independent test cases using hardware accelerators \citep{harding2007fast_springer, langdon2019continuous}, to application of primary-subordinate/controller-responder parallelism to delegate costly fitness evaluations of single individuals \citep{cantu2001master,miikkulainen2019evolving}.

Fully decentralized, highly-distributed computation is also applied, such as island models \citep{bennett1999building,schulte2010genetic} and mesh-computing approaches \citep{ackley2018digital,moreno2021conduit}.
However, a major barrier to effective applications of the fully-distributed paradigm has been difficulty of observing system state at scale.
In the absence of a global population-level view, the course of evolution becomes much more challenging to study --- undercutting applications of large-scale evolutionary simulation systems as an experimental platform.
Among other instrumentations, including phylogenetic information can help boost observability by providing insight into the mode and tempo of evolution \citep{dolson2020interpreting}.

\subsubsection{Further Applications of Distributed Ancestry Trees}

Replication of digital artifacts pervades computing, and interest in understanding structure of copy trees extends far beyond digital evolution.
Classic examples include the branching reported on the routes through which digital misinformation and computer viruses propagate \citep{friggeri2014rumor,cohen1987computer}.
Studies in this realm have generally relied on centralized tracking, leaving
circumstances where this is not feasible largely underexplored.

Where it has been performed, decentralized copy tracking has proven quite informative.
Work by \cite{libennowell2008tracing} on chain email provides a rare example.
These researchers applied post hoc methods to estimate phylogenies of chain mail messages.
These phylogenies then served as a reference to tune agent-based models, ultimately yielding a remarkable elucidation of email user behavior and underlying social dynamics.
Interestingly, this study's reconstructions were solely enabled by a special peculiarity of the sampled messages: they were email petitions.
Thus, users would append their name to the list before forwarding it on --- a mechanism similar in broad strokes to hereditary stratigraphy.

As an aside, loose parallels between hereditary stratigraphy and visual sedimentation, a dynamic graphic representation that draws on a similar geological metaphor to visualize data streams by progressively stacking, compressing, and then fusing visualized data objects with age, also bear mention \citep{huron2013visual}.

Hereditary stratigraphic techniques enabled new possibilities to delve into hereto cloistered processes through which digital artifacts spread.
Peer-to-peer and federated social networks, which have recently enjoyed substantial upticks in popularity \citep{la2021understanding}, make a prime example.
Within social networks, the structure of share chains is key to understanding, detecting, and mitigating misinformation \citep{kucharski2016study,raponi2022fake} and predatory viral marketing (e.g., spamming and phishing attacks) \citep{guidi2018managing}.
However, existing work relies on the capacity for direct ancestry tracking afforded by monolithic platforms (e.g., Facebook, Twitter, etc.).
Hereditary stratigraphy could enable robust, semi-anonymized extraction of share chain diagnostics within decentralized social networks.

Substantial limitations on uses cases for hereditary stratigraphy should be noted, however.
Annotations predicate influence over the artifact copy process necessary to ensure generational updates.
Additionally, the methodology is susceptible to removal or spoofing of annotation data by antagonistic actors.
